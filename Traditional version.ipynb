{"cells":[{"cell_type":"markdown","source":["# Traditional PyTorch on Databricks\n\nThis notebook, which is supposed to run within a Databricks cluster, is complementary to the SparkTorch version notebook. Here, we present the same approach as seen from a viewpoint more common in educational/academic circles, where distributed learning and cloud computing is not so common. For this reason, we will still upload the data from the Blob Storage, but then we will load them into a Pandas dataframe and work without making actual use of Spark's engine."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"696ec25e-7afb-4087-ab88-87199aa6d852","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import numpy as np\n\n# Same as in the other notebook\ndlname = dbutils.secrets.get(scope = \"testtuts\", key = \"lakename\")\nservice_credential = dbutils.secrets.get(scope=\"testtuts\", key=\"adcredential\")\napplication_id = dbutils.secrets.get(scope=\"testtuts\", key=\"DBappID\")\ndirectory_id = dbutils.secrets.get(scope=\"testtuts\", key=\"DBtenantID\")\n\nspark.conf.set(\"fs.azure.account.auth.type.\"+dlname+\".dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.\"+dlname+\".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.\"+dlname+\".dfs.core.windows.net\", application_id)\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.\"+dlname+\".dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\"+dlname+\".dfs.core.windows.net\", \"https://login.microsoftonline.com/\"+directory_id+\"/oauth2/token\")\n\ncontainername = \"torchtut\"\n\n# Train dataset\ntrain_data_url = f\"abfss://{containername}@{dlname}.dfs.core.windows.net/train.csv\"\ntraindf = spark.read.format(\"csv\").load(train_data_url, inferSchema = True, header = True).toPandas().astype(np.int8)\n\n# Test dataset\ntest_data_url = f\"abfss://{containername}@{dlname}.dfs.core.windows.net/test.csv\"\ntestdf = spark.read.format(\"csv\").load(test_data_url, inferSchema = True, header = True).toPandas().astype(np.int8)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e793a510-85bc-4ca8-8af8-7ab5d35c5175","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As discussed, the `traindf` and `testdf` are now Pandas instead of Spark dataframes, so all of our data are collected in the machine's RAM and are not going to be processed in a distributed manner."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5cca586f-9a8a-4d3d-9e20-cc86dadbeb54","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["traindf.head(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47d8bd75-fff8-4b8f-bc97-799be14b2f03","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>pixel10</th>\n      <th>...</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n      <th>pixel784</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>pixel10</th>\n      <th>...</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n      <th>pixel784</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We proceed by performing the min-max scaling as before, only now using scikit-learn's MinMaxScaler."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d9e7075-6635-4783-9c41-8f655f5ee19f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler().fit(traindf.iloc[:,:784])\ntraindf.iloc[:,:784] = scaler.transform(traindf.iloc[:,:784])\ntestdf.iloc[:,:784] = scaler.transform(testdf.iloc[:,:784])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"463301cb-f2f1-4b59-875d-270f78449619","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The dataframes must now be split into feature and label dataframes and then further split in order to obtain a validation set for the model's training further down the road."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8efcab3c-2692-45f2-9fd8-bc9b528a2cae","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["x_train, y_train, x_test, y_test = traindf.iloc[:,:784], traindf.iloc[:,784], testdf.iloc[:,:784], testdf.iloc[:,784]\n\nfrom sklearn.model_selection import train_test_split\n\n# get 30% of train data for validation\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=42)\n\n# The following are done so that the CNN model accepts the data as 2D images\na, b = x_train.shape\nb = int(np.sqrt(b))\nx_train = x_train.to_numpy().reshape(a,b,b)\n\ny_train = y_train.to_numpy()\n\na, b = x_val.shape\nb = int(np.sqrt(b))\nx_val = x_val.to_numpy().reshape(a,b,b)\n\ny_val = y_val.to_numpy()\n\na, b = x_test.shape\nb = int(np.sqrt(b))\nx_test = x_test.to_numpy().reshape(a,b,b)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"abde23b9-0d3d-4364-be8b-b6087021006c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["At this point, we define a `Dataset` class, in order to load our datasets into PyTorch loaders."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65935ffe-a5a5-4166-a090-42c6bce582a0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nclass CustomDatasetNN(Dataset):\n    def __init__(self, feats, labels):\n        self.feats = feats\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.feats)\n\n    def __getitem__(self, item):\n        img = self.feats[item]\n        tensoring = transforms.ToTensor()\n\n        img = tensoring(img).float()\n        return img, self.labels[item]\n    \ntrain_data = CustomDatasetNN(x_train,y_train)\ntrain_loader = DataLoader(train_data,batch_size=300,shuffle=True)\n\nval_data = CustomDatasetNN(x_val,y_val)\nval_loader = DataLoader(val_data,batch_size=300,shuffle=True)\n\ntest_data = CustomDatasetNN(x_test,y_test)\ntest_loader = DataLoader(test_data,batch_size=1,shuffle=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"210e5b7a-af27-4644-90b3-18f2d212e3ae","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that this is done, we may define the model as in the SparkTorch notebook. However, in this case we must also take care of explicitly defining the training, validation, earlystopping, etc. routines, as we did with the data-loading routines."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d83e305f-b071-4471-9ca6-fbe8c3f0be2d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["class CNNModel(nn.Module):\n    def __init__(self, input_height, input_width, conv_channels, kernels, maxpools, lin_channels, dropout):\n        super(CNNModel, self).__init__()\n        self.num_conv_layers = len(kernels)\n        \n        seq = []\n        for i in range(self.num_conv_layers):\n            seq.append(nn.Conv2d(in_channels=conv_channels[i], \n                                 out_channels=conv_channels[i+1],\n                                 kernel_size=kernels[i], stride=1, padding=1))\n            seq.append(nn.ReLU())\n            seq.append(nn.MaxPool2d(kernel_size=maxpools[i]))\n            \n        # Flatten the output of the final convolution layer\n        seq.append(nn.Flatten())\n        \n        convolutions = nn.Sequential(*seq)\n        \n        # Calculation of first linear layer dimensions\n        # We build an empty tensor of appropriate size and let him go through\n        # the above sequence, in order to calculate the output's size automatically\n        first_lin = convolutions(torch.empty(1,conv_channels[0],input_height,input_width)).size(-1)\n        \n        self.num_lin_layers = len(lin_channels)\n        for i in range(self.num_lin_layers):\n            if i == self.num_lin_layers-1:\n                seq.append(nn.Linear(lin_channels[i-1], lin_channels[i]))\n                break\n            elif i == 0:\n                seq.append(nn.Linear(first_lin, lin_channels[i]))\n            else:\n                seq.append(nn.Linear(lin_channels[i-1], lin_channels[i]))\n            seq.append(nn.ReLU())\n            seq.append(nn.Dropout(dropout))\n                \n        self.fitter = nn.Sequential(*seq)\n\n    def forward(self, x):\n        out = self.fitter(x)\n        return out\n    \ndef load_backbone_from_checkpoint(model, checkpoint_path):\n    model.load_state_dict(torch.load(checkpoint_path))\n    \n# adapted code from this repository: https://github.com/Bjarten/early-stopping-pytorch\nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'Validation loss increase spotted. Early stopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss\n\ndef training_loop(model, train_dataloader, optimizer):\n    model.train()\n    batch_losses = []\n            \n    for batch in train_dataloader:\n        x_batch, y_batch = batch\n                \n        x_batch, y_batch = x_batch.float(), y_batch.type(torch.LongTensor)\n                \n        # Clear the previous gradients first\n        optimizer.zero_grad()\n        \n        # forward pass\n        yhat = model(x_batch) # No unpacking occurs in CNNs\n        \n        # loss calculation\n        loss = loss_function(yhat, y_batch)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        \n        batch_losses.append(loss.data.item())\n        \n    train_loss = np.mean(batch_losses)\n\n    return train_loss\n\ndef validation_loop(model, val_dataloader):\n    \n    model.eval()\n    batch_losses = []\n    \n    for batch in val_dataloader:\n        x_batch, y_batch = batch\n                \n        x_batch, y_batch = x_batch.float(), y_batch.type(torch.LongTensor)\n        \n        yhat = model(x_batch)\n        \n        loss = loss_function(yhat, y_batch)\n        \n        batch_losses.append(loss.data.item())\n        \n    val_loss = np.mean(batch_losses)\n\n    return val_loss\n\n\ndef train(model, train_dataloader, val_dataloader, optimizer, epochs, patience=-1):\n\n    train_losses = []\n    val_losses = []\n    print(f\"Initiating CNN training.\")\n    model_path = f'CNN.pt'\n    checkpoint_path = 'checkpoint.pt'\n        \n    if patience != -1:\n        early_stopping = EarlyStopping(patience=patience, verbose=False, path=checkpoint_path)\n\n    for epoch in range(epochs):\n        \n        # Training loop\n        train_loss = training_loop(model, train_dataloader, optimizer)    \n        train_losses.append(train_loss)\n\n        # Validation loop\n        with torch.no_grad():\n\n            val_loss = validation_loop(model, val_dataloader)\n            val_losses.append(val_loss)\n\n        if patience != -1:\n            early_stopping(val_loss, model)\n\n            if early_stopping.early_stop:\n                print(\"Patience limit reached. Early stopping and going back to last checkpoint.\")\n                break\n\n    if patience != -1 and early_stopping.early_stop == True:\n        load_backbone_from_checkpoint(model,checkpoint_path)        \n\n    torch.save(model.state_dict(), model_path)\n\n    print(f\"CNN training finished.\\n\")\n    \n    return train_losses, val_losses\n    \ndef evaluate(model, test_dataloader):\n    model.eval()\n    predictions = []\n    labels = []\n    \n    with torch.no_grad():\n        for batch in test_dataloader:\n            \n            x_batch, y_batch = batch\n                \n            x_batch, y_batch = x_batch.float(), y_batch.type(torch.LongTensor)\n            \n            yhat = model(x_batch)\n            \n            # Calculate the index of the maximum argument\n            yhat_idx = torch.argmax(yhat, dim=1)\n            \n            predictions.append(yhat_idx.cpu().numpy())\n            labels.append(y_batch.cpu().numpy())\n    \n    return predictions, labels  # Return the model predictions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"891c9748-8cca-4389-8e83-ef405a47b5c4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["At this point, we may proceed with the training of the model, using the same set of parameters that were used in the SparkTorch notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16353bb1-fd55-4cdc-a459-51593491cecc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["input_height, input_width = b, b\nconv_channels = [1,4,16,32,64]\nkernels = [3,3,3,3]\nmaxpools = [2,2,2,2]\nlin_channels = [128,64,10]\ndropout = 0.1\nlearning_rate = 0.001\npatience = 20\n\nepochs = 150\n\nmodel = CNNModel(input_height = input_height, input_width = input_width,\n                 conv_channels = conv_channels, kernels = kernels, maxpools = maxpools,\n                 lin_channels = lin_channels, dropout = dropout)\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n# Train the model\nt_losses, v_losses = train(model, train_loader, val_loader, optimizer, epochs, patience=patience)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46acce76-35f3-415c-b202-f6e6987285a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Initiating CNN training.\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 4 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 4 out of 20\nValidation loss increase spotted. Early stopping counter: 5 out of 20\nValidation loss increase spotted. Early stopping counter: 6 out of 20\nValidation loss increase spotted. Early stopping counter: 7 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 4 out of 20\nValidation loss increase spotted. Early stopping counter: 5 out of 20\nValidation loss increase spotted. Early stopping counter: 6 out of 20\nValidation loss increase spotted. Early stopping counter: 7 out of 20\nValidation loss increase spotted. Early stopping counter: 8 out of 20\nValidation loss increase spotted. Early stopping counter: 9 out of 20\nValidation loss increase spotted. Early stopping counter: 10 out of 20\nValidation loss increase spotted. Early stopping counter: 11 out of 20\nValidation loss increase spotted. Early stopping counter: 12 out of 20\nValidation loss increase spotted. Early stopping counter: 13 out of 20\nValidation loss increase spotted. Early stopping counter: 14 out of 20\nValidation loss increase spotted. Early stopping counter: 15 out of 20\nValidation loss increase spotted. Early stopping counter: 16 out of 20\nValidation loss increase spotted. Early stopping counter: 17 out of 20\nValidation loss increase spotted. Early stopping counter: 18 out of 20\nValidation loss increase spotted. Early stopping counter: 19 out of 20\nValidation loss increase spotted. Early stopping counter: 20 out of 20\nPatience limit reached. Early stopping and going back to last checkpoint.\nCNN training finished.\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Initiating CNN training.\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 4 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 4 out of 20\nValidation loss increase spotted. Early stopping counter: 5 out of 20\nValidation loss increase spotted. Early stopping counter: 6 out of 20\nValidation loss increase spotted. Early stopping counter: 7 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 1 out of 20\nValidation loss increase spotted. Early stopping counter: 2 out of 20\nValidation loss increase spotted. Early stopping counter: 3 out of 20\nValidation loss increase spotted. Early stopping counter: 4 out of 20\nValidation loss increase spotted. Early stopping counter: 5 out of 20\nValidation loss increase spotted. Early stopping counter: 6 out of 20\nValidation loss increase spotted. Early stopping counter: 7 out of 20\nValidation loss increase spotted. Early stopping counter: 8 out of 20\nValidation loss increase spotted. Early stopping counter: 9 out of 20\nValidation loss increase spotted. Early stopping counter: 10 out of 20\nValidation loss increase spotted. Early stopping counter: 11 out of 20\nValidation loss increase spotted. Early stopping counter: 12 out of 20\nValidation loss increase spotted. Early stopping counter: 13 out of 20\nValidation loss increase spotted. Early stopping counter: 14 out of 20\nValidation loss increase spotted. Early stopping counter: 15 out of 20\nValidation loss increase spotted. Early stopping counter: 16 out of 20\nValidation loss increase spotted. Early stopping counter: 17 out of 20\nValidation loss increase spotted. Early stopping counter: 18 out of 20\nValidation loss increase spotted. Early stopping counter: 19 out of 20\nValidation loss increase spotted. Early stopping counter: 20 out of 20\nPatience limit reached. Early stopping and going back to last checkpoint.\nCNN training finished.\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finally, we obtain the results after the model's evaluation on the test set."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"41291378-be6d-4f65-91fa-d7b12a3484ee","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Evaluate the model\npredictions, labels = evaluate(model, test_loader)\n\ny_pred = np.concatenate(predictions, axis=0)\ny_true = np.concatenate(labels, axis=0)\n\nfrom sklearn.metrics import accuracy_score\n\nacc = accuracy_score(y_true, y_pred)\n\nprint(f\"The final model's accuracy is {acc*100:.4f}%\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"793c010b-99a8-4e61-b360-8528e38a5ccd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The final model's accuracy is 93.4600%\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The final model's accuracy is 93.4600%\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89f19692-bc93-4ef6-8604-32c5ac633c0a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Traditional PyTorch on Databricks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1877122803467285}},"nbformat":4,"nbformat_minor":0}
